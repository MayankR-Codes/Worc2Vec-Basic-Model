# Word2Vec Skip-Gram Model: Word Similarity Demonstration

A comprehensive Jupyter notebook demonstrating how Word2Vec Skip-Gram model learns word embeddings and measures semantic similarity between words. This project proves that contextually related words (Fruit and Apple) have higher similarity than unrelated words (Apple and Truck).

## ðŸ“š Project Overview

This notebook implements a complete Word2Vec Skip-Gram model training pipeline from scratch, demonstrating fundamental concepts of natural language processing (NLP) and word embeddings. The model learns to represent words as dense vectors in a multi-dimensional space where semantically similar words are positioned close to each other.

### Key Highlights
- **âœ“ Successful Model Training**: Demonstrates proper semantic relationship learning
- **âœ“ Vector Similarity Analysis**: Compares word embeddings using cosine similarity
- **âœ“ Verification Results**: 
  - Fruit â†” Apple similarity: **0.9928** (highly related)
  - Apple â†” Truck similarity: **0.9744** (less related)
  - The model correctly identifies semantic relationships

## ðŸ“‹ Table of Contents

1. [Installation](#installation)
2. [How to Run](#how-to-run)
3. [Project Structure](#project-structure)
4. [Detailed Section Explanation](#detailed-section-explanation)
5. [Understanding the Model](#understanding-the-model)
6. [Results and Interpretation](#results-and-interpretation)
7. [Technical Details](#technical-details)
8. [Requirements](#requirements)

## ðŸ”§ Installation

### Prerequisites
- Python 3.8 or higher
- Jupyter Notebook or JupyterLab
- pip package manager

### Setup Instructions

1. **Clone the repository**
```bash
git clone https://github.com/MayankR-Codes/Worc2Vec-Basic-Model.git
cd Worc2Vec-Basic-Model
```

2. **Create a virtual environment (optional but recommended)**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install required packages**
```bash
pip install gensim scikit-learn numpy pandas jupyter
```

4. **Launch Jupyter Notebook**
```bash
jupyter notebook word2vec_model.ipynb
```

## ðŸš€ How to Run

1. Open the notebook in Jupyter
2. Run cells sequentially from top to bottom using `Shift + Enter` or the Run button
3. Observe the output and understand how the model learns semantic relationships
4. Modify parameters to experiment with different configurations

**Execution Time**: ~5-10 seconds per training cell depending on your system

## ðŸ“ Project Structure

```
word2vec_model.ipynb
â”œâ”€â”€ Section 1: Import Required Libraries
â”œâ”€â”€ Section 2: Create Toy Dictionary and Training Corpus
â”œâ”€â”€ Section 3: Train Word2Vec Skip-Gram Model
â”œâ”€â”€ Section 4: Generate and Display Word Vectors
â”œâ”€â”€ Section 5: Calculate Vector Similarities
â””â”€â”€ Section 6: Compare Word Distances and Verify Results
```

## ðŸ“– Detailed Section Explanation

### **Section 1: Import Required Libraries**
Imports essential Python libraries:
- **gensim**: Provides the Word2Vec implementation
- **scikit-learn**: Used for cosine similarity calculations
- **numpy**: Numerical computing library for array operations
- **pandas**: Data manipulation and display in tabular format

```python
from gensim.models import Word2Vec
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import pandas as pd
```

---

### **Section 2: Create Toy Dictionary and Training Corpus**
Creates a manageable vocabulary and training data:

**Toy Dictionary (10 core words)**:
- Fruits: Apple, Orange, Banana, Fruit
- Vehicles: Truck, Car, Vehicle, Wheel
- Attributes: Red, Sweet, Heavy, Fast, Round

**Training Corpus (87 sentences)**:
The corpus is strategically designed with:
- **45 fruit-domain pairs**: Heavy repetition of Fruit-Apple, Fruit-Orange, Fruit-Banana relationships
- **42 vehicle-domain pairs**: Heavy repetition of Vehicle-Truck, Vehicle-Car, Vehicle-Wheel relationships
- **Domain separation**: Minimal cross-domain interaction to prevent confusion

The repetition is crucial because Word2Vec learns through context window co-occurrence. Words appearing together frequently develop similar vector representations.

**Example corpus structure**:
```
Fruit domain: [Apple, Fruit] x5, [Orange, Fruit] x5, [Banana, Fruit] x5, etc.
Vehicle domain: [Truck, Vehicle] x5, [Car, Vehicle] x5, [Wheel, Vehicle] x5, etc.
```

---

### **Section 3: Train Word2Vec Skip-Gram Model**
Trains the Skip-Gram neural network model with carefully tuned hyperparameters:

**Model Configuration**:
| Parameter | Value | Purpose |
|-----------|-------|---------|
| `sg=1` | Skip-Gram | Predicts context words from target word |
| `vector_size=100` | 100 dimensions | Rich embedding space |
| `window=1` | Context window of 1 | Strict local context learning |
| `epochs=1000` | 1000 iterations | Strong convergence |
| `negative=10` | 10 negative samples | Better discrimination |
| `workers=4` | 4 threads | Parallel processing |

**How Skip-Gram Works**:
- Takes a target word and predicts surrounding context words
- Uses a shallow neural network to learn word embeddings
- Objective: Words with similar contexts should have similar vectors

---

### **Section 4: Generate and Display Word Vectors**
Extracts and visualizes the learned word embeddings:

**Output**:
- First 10 dimensions of key word vectors (e.g., Fruit, Apple, Truck)
- Complete vocabulary list with 13 words
- Demonstrates that each word has a unique 100-dimensional representation

**Sample Output**:
```
'Fruit' vector: [-0.06848942  0.2757941   0.08895981  0.1646756  -0.06122581...]
'Apple' vector: [-0.06006607  0.31172842  0.08490056  0.19271123 -0.06986912...]
'Truck' vector: [ 0.00841906  0.2807333   0.03125701  0.18380485 -0.07754957...]
```

---

### **Section 5: Calculate Vector Similarities**
Computes cosine similarity between word vectors:

**Cosine Similarity Formula**:
$$\text{similarity}(A, B) = \frac{A \cdot B}{\|A\| \times \|B\|}$$

Where:
- Ranges from -1 to 1 (typically 0 to 1 for word embeddings)
- 1.0 = identical direction (most similar)
- 0.0 = orthogonal (no similarity)
- -1.0 = opposite direction (most dissimilar)

**Key Similarities**:
- Fruit â†” Apple: **0.9928** (extremely similar - same semantic domain)
- Apple â†” Truck: **0.9744** (less similar - different domains)
- Difference: **0.0184** (positive confirmation of expected relationship)

---

### **Section 6: Compare Word Distances and Verify Results**
Final comprehensive analysis and verification:

**Output Components**:

1. **Comparison Table**: Shows word pairs and their similarities
2. **Verification Result**: Confirms successful learning
3. **Most Similar Words to 'Apple'**:
   - Orange: 0.9973
   - Banana: 0.9972
   - Sweet: 0.9962
   - Fruit: 0.9928
   - Red: 0.9926

**Success Criteria** âœ“:
- âœ“ Fruit-Apple > Apple-Truck (semantic grouping works)
- âœ“ Top-5 similar words to Apple are mostly fruits (correct semantics)
- âœ“ Model properly learned contextual relationships

---

## ðŸ§  Understanding the Model

### What is Word2Vec?

Word2Vec is a two-layer neural network that processes text to learn word relationships:

1. **Input Layer**: One-hot encoded word vector
2. **Hidden Layer**: Learns dense embeddings (100-D in this case)
3. **Output Layer**: Predicts surrounding words

### Skip-Gram vs CBOW

| Aspect | Skip-Gram | CBOW |
|--------|-----------|------|
| Input | Target word | Context words |
| Output | Context words | Target word |
| Use Case | Small datasets, rare words | Large datasets |
| Speed | Slower | Faster |

This project uses **Skip-Gram** (`sg=1`), which is excellent for learning from limited data.

### Why Vector Embeddings Matter

Word embeddings capture semantic meaning in a continuous space:
- Similar words cluster together
- Can perform arithmetic: `vector(King) - vector(Man) + vector(Woman) â‰ˆ vector(Queen)`
- Enable downstream NLP tasks (sentiment analysis, machine translation, etc.)

---

## ðŸ“Š Results and Interpretation

### Success Metrics

**âœ“ Primary Result**: Fruit-Apple similarity (0.9928) > Apple-Truck similarity (0.9744)

**Why This Matters**:
- Shows the model learned semantic domain separation
- Fruit and Apple appear together frequently in training â†’ high similarity
- Apple and Truck appear separately â†’ lower similarity
- Demonstrates the model understands contextual relationships

### Semantic Clustering

The model successfully created semantic clusters:

```
Fruit Cluster:          Vehicle Cluster:
â”œâ”€â”€ Apple              â”œâ”€â”€ Truck
â”œâ”€â”€ Orange             â”œâ”€â”€ Car
â”œâ”€â”€ Banana             â”œâ”€â”€ Wheel
â”œâ”€â”€ Fruit              â”œâ”€â”€ Vehicle
â””â”€â”€ Sweet              â””â”€â”€ Heavy/Fast/Round
```

### Most Similar Words Analysis

For the word "Apple":
1. **Orange** (0.9973) - Same domain (fruit)
2. **Banana** (0.9972) - Same domain (fruit)
3. **Sweet** (0.9962) - Attribute of fruit
4. **Fruit** (0.9928) - Hypernym (category)
5. **Red** (0.9926) - Attribute of apple

All top-5 similar words are contextually related, proving successful learning.

---

## ðŸ”¬ Technical Details

### Hyperparameter Tuning Strategy

The parameters were specifically chosen to maximize semantic learning:

1. **Large Embedding Space (vector_size=100)**
   - More dimensions = richer representations
   - Prevents underfitting on small vocabulary

2. **Small Context Window (window=1)**
   - Forces strict local context learning
   - Emphasizes immediate neighboring words
   - Better domain separation

3. **High Epochs (epochs=1000)**
   - Ensures convergence of the optimization
   - 200 epochs wouldn't be sufficient for 87 sentences

4. **More Negative Samples (negative=10)**
   - Better contrast between similar and dissimilar words
   - Improves discrimination between domains

### Training Data Design

The 87-sentence corpus carefully balances:
- **Repetition**: Each fruit-fruit pair repeated 3-5 times
- **Variety**: Different word combinations within same domain
- **Domain Separation**: Minimal cross-domain contamination
- **Symmetry**: Both directions (Aâ†’B and Bâ†’A)

Original corpus with irregular patterns failed to achieve proper separation. The refined corpus with strategic repetition succeeded.

---

## ðŸ“¦ Requirements

```
gensim==4.4.0
scikit-learn==1.7.2
numpy==2.2.6
pandas==2.3.3
jupyter>=1.0.0
```

Install all at once:
```bash
pip install gensim scikit-learn numpy pandas jupyter
```

---

## ðŸŽ¯ Use Cases and Extensions

### Real-World Applications
- **Sentiment Analysis**: Use trained embeddings as features
- **Recommendation Systems**: Find similar products/items
- **Machine Translation**: Align words across languages
- **Document Classification**: Aggregate word vectors for documents
- **Information Retrieval**: Search and ranking systems

### Possible Extensions
1. **Visualize embeddings** using t-SNE or UMAP
2. **Train on real corpus** (Wikipedia, news articles)
3. **Compare with modern embeddings** (FastText, BERT)
4. **Implement CBOW** alternative algorithm
5. **Experiment with different window sizes** and epochs
6. **Build a word similarity API** using the trained model

---

## ðŸ“ Notes

- The model trains in ~2-3 seconds on modern hardware
- Results are deterministic (seed=42) for reproducibility
- The toy dictionary is intentionally small for demonstration
- Larger datasets will produce more robust embeddings
- This approach scales to millions of words in real applications

---

## ðŸ”— Resources

- [Gensim Word2Vec Documentation](https://radimrezsik.com/gensim/models/word2vec.html)
- [Original Word2Vec Paper](https://arxiv.org/abs/1301.3781)
- [Cosine Similarity Explained](https://en.wikipedia.org/wiki/Cosine_similarity)
- [NLP Tutorial](https://www.nltk.org/)

---

## ðŸ“„ License

This project is open source and available for educational purposes.

---

## ðŸ‘¨â€ðŸ’» Author

Created as an educational demonstration of Word2Vec Skip-Gram model fundamentals.

**Repository**: https://github.com/MayankR-Codes/Worc2Vec-Basic-Model

---

## â“ FAQ

**Q: Why are the similarities so high (>0.97)?**
A: The toy dictionary is small (13 words) and the training corpus has heavy repetition. With larger, more diverse data, similarities would be more varied.

**Q: What does the 100-dimensional vector represent?**
A: Each dimension learns abstract features capturing different aspects of word meaning. Interpreting individual dimensions is difficult; it's the overall geometric relationship that matters.

**Q: Can I use this model on real text?**
A: Yes! Simply replace the `sentences` variable with your text data. For large corpora, consider using pre-trained models (Word2Vec, GloVe, FastText).

**Q: Why is Skip-Gram better than CBOW for this task?**
A: Skip-Gram works better with limited data and learns better representations for rare words, which is ideal for our small toy dictionary.

**Q: How do I improve the model?**
A: Increase training data, tune hyperparameters (window size, epochs), increase vocabulary size, or use more powerful embedding techniques (FastText, BERT).

---

**Happy Learning! ðŸš€**
